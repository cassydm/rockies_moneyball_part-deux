{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrate app into UI interface: Gradio\n",
    "\n",
    "- **input fields**: \n",
    "    - text box for user to describe current game information\n",
    "        - optional: voice-to-text capability\n",
    "    - optional: dedicated input fields for current game information (pitcher name, batter name, who is on base, fielding alignment, etc)\n",
    "- **output fields**:\n",
    "    - LLM Chat with history\n",
    "    - optional: toggle on/off history visibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all dependencies and libraries\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "# from transformers import pipeline\n",
    "import gradio as gr\n",
    "\n",
    "\n",
    "# Load environment variables.\n",
    "load_dotenv()\n",
    "# Set the model name for our LLMs.\n",
    "GEMINI_MODEL = \"gemini-1.5-flash\"\n",
    "# Store the API key in a variable.\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "\n",
    "# instatiate the LLM\n",
    "llm = ChatGoogleGenerativeAI(google_api_key=GEMINI_API_KEY, model=GEMINI_MODEL, temperature=0.5)\n",
    "\n",
    "\n",
    "# create a template for the chatbot personality\n",
    "template = \"\"\"\n",
    "You are a baseball coach. Answer only questions that would pertaining to baseball.\n",
    "If the human asks questions not related to baseball, remind them that your job is to help\n",
    "them learn answer baseball questions, and ask them for a question on that topic. If they ask a question which\n",
    "there is not enough information to answer, tell them you don't know and don't make up an \n",
    "answer.\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "Human: {input}\n",
    "AI Assistant:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"history\", \"input\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "\n",
    "# create conversation chain with memory\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "conversation = ConversationChain(llm=llm, memory=memory, prompt=prompt,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for quick tests:\n",
    "\n",
    "def placeholder_fn(query, history):\n",
    "    # add user's message to conversation history\n",
    "    history.append((\"User\", query))\n",
    "    # get response from AI\n",
    "    result = conversation.predict(input=query)\n",
    "    # add AI response to conversation history\n",
    "    history.append((\"AI\", result))\n",
    "    # format conversation history for display\n",
    "    formatted_history = \"\\n\".join([f\"{sender}: {msg}\" for sender, msg in history])\n",
    "\n",
    "    return formatted_history, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stretch goal: voice to text functionality\n",
    "add this functionality after core functionality is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # add voice-to-text input\n",
    "# import torch\n",
    "# from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "# from datasets import load_dataset\n",
    "\n",
    "\n",
    "# device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "# model_id = \"openai/whisper-large-v3\"\n",
    "\n",
    "# model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "#     model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    "# )\n",
    "# model.to(device)\n",
    "\n",
    "# processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "# pipe = pipeline(\n",
    "#     \"automatic-speech-recognition\",\n",
    "#     model=model,\n",
    "#     tokenizer=processor.tokenizer,\n",
    "#     feature_extractor=processor.feature_extractor,\n",
    "#     torch_dtype=torch_dtype,\n",
    "#     device=device,\n",
    "# )\n",
    "\n",
    "# dataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\n",
    "# sample = dataset[0][\"audio\"]\n",
    "\n",
    "# result = pipe(sample)\n",
    "# print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def transcribe(audio):\n",
    "#     if audio is not None: \n",
    "#         result = transcriber(audio)\n",
    "#         return result['text']\n",
    "#     else: \n",
    "#         return \"\"\n",
    "\n",
    "# def process_input(audio, text, history):\n",
    "#     message = transcribe(audio) if audio is not None else text\n",
    "#     return placeholder_fn(message, history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrate LLM/Transformer function(s) into Gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradio: using default format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\elcoo\\anaconda3\\Lib\\site-packages\\gradio\\interface.py:393: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# gradio app\n",
    "\n",
    "chat_app = gr.Interface(\n",
    "    fn = placeholder_fn, \n",
    "    inputs = [\n",
    "        # gr.Audio(source=\"microphone\", type=\"filepath\", label=\"Speak your message (optional)\"),\n",
    "        gr.Textbox(lines = 2, label = \"Or type your message here\"),\n",
    "        gr.State([])\n",
    "    ],\n",
    "    outputs = [\n",
    "        gr.Textbox(lines = 10, label=\"Coach AI response:\", show_copy_button=True),\n",
    "        gr.State()\n",
    "    ],\n",
    "    title=\"Coach AI\",\n",
    "    description=\"Chat with an AI using your voice or by typing. The AI remembers your conversation history.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7867\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7867/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# launch app locally\n",
    "\n",
    "chat_app.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradio: Using Blocks for formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theme Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\elcoo\\anaconda3\\Lib\\site-packages\\gradio\\components\\chatbot.py:222: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7868\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7868/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.themes.builder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### App with applied theming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\elcoo\\anaconda3\\Lib\\site-packages\\gradio\\components\\chatbot.py:222: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7897\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7897/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "theme = gr.themes.Ocean(\n",
    "    primary_hue=\"indigo\",\n",
    "    secondary_hue=\"fuchsia\",\n",
    ")\n",
    "\n",
    "with gr.Blocks(theme=theme) as chat_app:\n",
    "    # Add an image to the top center\n",
    "    gr.Image(\"images\\colorado-rockies.png\", show_label=False, container=False, height=200)\n",
    "    gr.HTML(\"\"\"\n",
    "    <style>\n",
    "    .center-text { text-align: center; }\n",
    "    </style>\n",
    "            \"\"\")\n",
    "     \n",
    "\n",
    "    gr.HTML('<h1 class=\"center-text\">Moneyball Part Deux, part 3</h1>')\n",
    "    gr.HTML('<p class=\"center-text\">Chat with your personal AI Coach about anything related to baseball.</p>')\n",
    "    \n",
    "    chatbot = gr.Chatbot()\n",
    "    chat_input = gr.MultimodalTextbox(\n",
    "        interactive=True, \n",
    "        placeholder=\"Enter message or upload file...\", \n",
    "        show_label=False,\n",
    "    )\n",
    "\n",
    "    clear = gr.Button(\"Clear\", variant=\"primary\")\n",
    "\n",
    "    def user(user_message, history):\n",
    "        return \"\", history + [[user_message, None]]\n",
    "\n",
    "    def bot(history):\n",
    "        bot_message, updated_history = placeholder_fn(history[-1][0], history[:-1])\n",
    "        history[-1][1] = bot_message\n",
    "        return history\n",
    "\n",
    "    chat_input.submit(user, [chat_input, chatbot], [chat_input, chatbot], queue=False).then(\n",
    "        bot, chatbot, chatbot\n",
    "    )\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chat_app.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
