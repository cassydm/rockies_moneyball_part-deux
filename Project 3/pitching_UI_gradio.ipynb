{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrate app into UI interface: Gradio\n",
    "\n",
    "- **input fields**: \n",
    "    - text box for user to describe current game information\n",
    "        - optional: voice-to-text capability\n",
    "    - optional: dedicated input fields for current game information (pitcher name, batter name, who is on base, fielding alignment, etc)\n",
    "- **output fields**:\n",
    "    - LLM Chat with history\n",
    "    - optional: toggle on/off history visibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all dependencies and libraries\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from transformers import pipeline\n",
    "import gradio as gr\n",
    "\n",
    "\n",
    "# Load environment variables.\n",
    "load_dotenv()\n",
    "# Set the model name for our LLMs.\n",
    "GEMINI_MODEL = \"gemini-1.5-flash\"\n",
    "# Store the API key in a variable.\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "\n",
    "# instatiate the LLM\n",
    "llm = ChatGoogleGenerativeAI(google_api_key=GEMINI_API_KEY, model=GEMINI_MODEL, temperature=0.5)\n",
    "\n",
    "\n",
    "# create a template for the chatbot personality\n",
    "template = \"\"\"\n",
    "You are a baseball coach. Answer only questions that would pertaining to baseball.\n",
    "If the human asks questions not related to baseball, remind them that your job is to help\n",
    "them learn answer baseball questions, and ask them for a question on that topic. If they ask a question which\n",
    "there is not enough information to answer, tell them you don't know and don't make up an \n",
    "answer.\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "Human: {input}\n",
    "AI Assistant:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"history\", \"input\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "\n",
    "# create conversation chain with memory\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "conversation = ConversationChain(llm=llm, memory=memory, prompt=prompt,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for quick tests:\n",
    "\n",
    "def placeholder_fn(query, history):\n",
    "    # add user's message to conversation history\n",
    "    history.append((\"User\", query))\n",
    "    # get response from AI\n",
    "    result = conversation.predict(input=query)\n",
    "    # add AI response to conversation history\n",
    "    history.append((\"AI\", result))\n",
    "    # format conversation history for display\n",
    "    formatted_history = \"\\n\".join([f\"{sender}: {msg}\" for sender, msg in history])\n",
    "\n",
    "    return formatted_history, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # add voice-to-text input\n",
    "# import torch\n",
    "# from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "# from datasets import load_dataset\n",
    "\n",
    "\n",
    "# device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "# model_id = \"openai/whisper-large-v3\"\n",
    "\n",
    "# model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "#     model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    "# )\n",
    "# model.to(device)\n",
    "\n",
    "# processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "# pipe = pipeline(\n",
    "#     \"automatic-speech-recognition\",\n",
    "#     model=model,\n",
    "#     tokenizer=processor.tokenizer,\n",
    "#     feature_extractor=processor.feature_extractor,\n",
    "#     torch_dtype=torch_dtype,\n",
    "#     device=device,\n",
    "# )\n",
    "\n",
    "# dataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\n",
    "# sample = dataset[0][\"audio\"]\n",
    "\n",
    "# result = pipe(sample)\n",
    "# print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def transcribe(audio):\n",
    "#     if audio is not None: \n",
    "#         result = transcriber(audio)\n",
    "#         return result['text']\n",
    "#     else: \n",
    "#         return \"\"\n",
    "\n",
    "# def process_input(audio, text, history):\n",
    "#     message = transcribe(audio) if audio is not None else text\n",
    "#     return placeholder_fn(message, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradio UI \n",
    "\n",
    "chat_app = gr.Interface(\n",
    "    fn = placeholder_fn, \n",
    "    inputs = [\n",
    "        # gr.Audio(source=\"microphone\", type=\"filepath\", label=\"Speak your message (optional)\"),\n",
    "        gr.Textbox(lines = 2, label = \"Or type your message here\"),\n",
    "        gr.State([])\n",
    "    ],\n",
    "    outputs = [\n",
    "        gr.Textbox(lines = 10, label=\"Coach AI response:\", show_copy_button=True),\n",
    "        gr.State()\n",
    "    ],\n",
    "    title=\"Coach AI\",\n",
    "    description=\"Chat with an AI using your voice or by typing. The AI remembers your conversation history.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7866\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7866/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are a baseball coach. Answer only questions that would pertaining to baseball.\n",
      "If the human asks questions not related to baseball, remind them that your job is to help\n",
      "them learn answer baseball questions, and ask them for a question on that topic. If they ask a question which\n",
      "there is not enough information to answer, tell them you don't know and don't make up an \n",
      "answer.\n",
      "\n",
      "Current conversation:\n",
      "[]\n",
      "Human: hi my name is liz\n",
      "AI Assistant:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are a baseball coach. Answer only questions that would pertaining to baseball.\n",
      "If the human asks questions not related to baseball, remind them that your job is to help\n",
      "them learn answer baseball questions, and ask them for a question on that topic. If they ask a question which\n",
      "there is not enough information to answer, tell them you don't know and don't make up an \n",
      "answer.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content='hi my name is liz', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hi Liz!  I'm ready to answer any baseball questions you have.  What would you like to know? \\n\", additional_kwargs={}, response_metadata={})]\n",
      "Human: who are the starting pitchers for the 2024 colorado rockies?\n",
      "AI Assistant:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# launch app locally\n",
    "\n",
    "chat_app.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
